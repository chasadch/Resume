# Chapter 3: System Architecture

The autonomous drone system architecture follows a modular, layered design approach that enables component isolation, simplified testing, and future expandability. This architectural philosophy prioritizes clear interfaces between subsystems, allowing individual components to be developed, tested, and refined independently while maintaining compatibility with the overall system. The modular approach also facilitates future enhancements by allowing specific components to be upgraded or replaced without requiring wholesale system redesign. This chapter details the hardware platform, computational architecture, sensor configuration, software organization, and communication framework that collectively form the foundation of the autonomous drone system. The architectural decisions described here directly address several research gaps identified in the literature review, particularly those related to system integration and real-time performance in resource-constrained environments.

The hardware platform selection represents a critical foundation for the autonomous drone system, balancing computational capabilities against power consumption, weight, and cost constraints. After evaluating several embedded computing platforms, the Jetson Xavier NX was selected as the primary computing unit due to its optimal combination of GPU-accelerated processing power (384 CUDA cores and 48 Tensor cores), moderate power consumption (10-15W during typical operation), and reasonable weight (approximately 84g for the module alone). The platform provides 8GB of RAM and 16GB of eMMC storage, sufficient for running the perception, planning, and control algorithms without external memory requirements. The Xavier NX's hardware acceleration capabilities are particularly valuable for the computer vision components of the system, enabling real-time object detection while maintaining acceptable power consumption. The computing platform is integrated with a custom carrier board that provides power management, sensor interfaces, and communication links optimized for the drone application, reducing weight and complexity compared to standard development carrier boards.

The drone platform itself is based on a modified quadcopter frame with a 450mm motor-to-motor diagonal distance, providing sufficient payload capacity (approximately 800g beyond the base weight) while maintaining agility and reasonable flight time. The frame features a central mounting area for the computing system and primary sensors, with dedicated vibration-isolated platforms for inertial measurement units and sensitive camera equipment. Power is supplied by a 4S 5000mAh LiPo battery, providing approximately 20 minutes of flight time under typical operation conditions. The propulsion system consists of four 920KV brushless motors with 10-inch propellers, selected to balance thrust requirements (approximately 1.2kg per motor) with energy efficiency. Multiple power distribution systems separate the high-current motor circuits from the sensitive computing and sensor systems, reducing electromagnetic interference and providing stable, filtered power to critical components. The platform includes automatic battery monitoring with configurable low-voltage thresholds that trigger landing procedures when capacity falls below safe operating levels.

The sensor configuration integrates multiple modalities to create a comprehensive environmental perception system resistant to individual sensor limitations or failures. The primary visual perception system consists of three global-shutter cameras: a forward-facing 120° wide-angle camera for general navigation and obstacle detection, a downward-facing camera for ground tracking and landing area assessment, and a 30° narrow-angle forward camera for detailed inspection and long-range obstacle identification. This multi-camera approach addresses the trade-off between field of view and resolution, providing both broad environmental awareness and detailed object recognition capabilities. Depth perception is achieved through a combination of stereo vision processing between overlapping camera fields and a dedicated depth camera based on structured light technology, which provides reliable short to medium-range 3D information within a 60° field of view. Inertial sensing employs a primary 9-axis IMU (accelerometer, gyroscope, and magnetometer) for attitude estimation and motion tracking, with a secondary redundant IMU that enables consistency checking and fault detection. Global positioning integrates GPS/GNSS receivers with barometric pressure sensors for altitude estimation, creating a reliable positioning system with approximately ±2m horizontal accuracy and ±0.5m relative altitude accuracy in typical conditions.

The software architecture implements a hierarchical structure with distinct layers for hardware abstraction, perception, planning, and control, facilitating clear separation of concerns and modular development. The lowest layer, the Hardware Abstraction Layer (HAL), provides standardized interfaces to sensors and actuators, isolating higher-level components from hardware-specific details and enabling hardware changes without modifying application code. The Perception Layer processes sensor data to create environmental representations, including object detection through the YOLOv8 vision system, depth mapping from stereo and structured light sensors, and state estimation combining inertial and visual odometry data. The Planning Layer includes both global path planning based on mission objectives and local trajectory generation that considers immediate obstacles and constraints. The Control Layer implements nested control loops for position, velocity, and attitude stabilization, translating high-level trajectory commands into specific motor control signals. This layered architecture enables clear interfaces between components while allowing each layer to operate at appropriate update frequencies, from the 400Hz attitude control loop to the 10Hz global planning update rate.

The system implements a mixed-criticality approach, with safety-critical functions receiving prioritized resources and guaranteed execution times. The flight controller firmware handles the most critical tasks, including attitude stabilization and basic obstacle avoidance, operating in a real-time environment with deterministic performance. The companion computer (Jetson Xavier NX) manages higher-level functions including vision processing, path planning, and mission management, with critical processes assigned higher scheduling priorities and, where necessary, dedicated CPU cores. This division of responsibilities ensures that basic flight safety is maintained even if the more complex perception and planning systems experience delays or failures. The software architecture incorporates explicit failure handling at each level, with graceful degradation pathways that maintain core functionality when specific components encounter problems. For example, if the vision-based obstacle detection system fails, the architecture automatically falls back to ultrasonic distance sensors for basic obstacle avoidance, sacrificing sophisticated object classification but maintaining fundamental collision prevention capabilities.

The communication architecture establishes reliable data exchange between system components across multiple physical and logical channels. Internal communication between software components utilizes a publish-subscribe messaging framework based on ROS2 (Robot Operating System 2), selected for its flexible communication patterns and quality-of-service options that support both reliable and best-effort message delivery as appropriate for different data types. Critical control messages use deterministic communication paths with guaranteed bandwidth and bounded latency, while high-volume sensor data like camera feeds use best-effort delivery with efficient zero-copy methods where possible. External communication with the ground control station operates across redundant channels, including a primary 5.8GHz wireless link for high-bandwidth data and control, and a secondary 900MHz long-range link for basic telemetry and emergency commands. Both channels implement the MAVLink protocol for standardized message formatting and handling, with custom extensions for application-specific functionality. The communication system includes automatic channel selection based on signal quality, range requirements, and bandwidth needs, with graceful degradation as signal quality decreases. Encryption is applied to all control channels to prevent unauthorized access or interference, with AES-256 encryption for sensitive commands and lighter encryption for high-bandwidth video streams to balance security and performance.

The data management subsystem handles the collection, processing, storage, and distribution of information throughout the autonomous drone system. Sensor data undergoes multi-stage processing, beginning with hardware-level filtering and preprocessing (such as image debayering and initial noise reduction), followed by feature extraction and fusion with complementary sensors, and culminating in high-level representations such as object bounding boxes or occupancy maps. The system implements a multi-resolution approach to environmental representation, with detailed high-resolution mapping for the immediate surroundings (within 10 meters) and progressively coarser representations for more distant areas, balancing memory usage and computational requirements against the precision needed at different ranges. Persistent storage manages both operational data, such as maps and environment models, and system logs that record flight parameters, decisions, and sensor readings for post-mission analysis and system refinement. A standardized logging framework captures structured data with accurate timestamps, facilitating both automated analysis and manual inspection when necessary. The data management system also implements selective recording based on mission phases and detected events, automatically increasing the detail and frequency of data capture during complex maneuvers, anomalous conditions, or operator-flagged segments.

The power management system optimizes energy usage while ensuring reliable operation throughout the mission duration. Hardware-level power management includes dynamic voltage and frequency scaling for the computing platform, enabling performance adjustments based on current processing requirements. During computationally intensive operations, such as running multiple simultaneous detection models, the system allocates maximum power to the CPU and GPU cores; during less demanding phases, it reduces clock speeds and disables unnecessary processing units to extend battery life. Sensor management selectively activates and deactivates perception sensors based on mission requirements and environmental conditions, for example, reducing camera frame rates in stable, obstacle-free environments or activating long-range sensors only when planning high-speed segments. Software-level optimizations include workload scheduling that balances processing demands across time, avoiding simultaneous execution of multiple intensive tasks when possible. The system continuously monitors power consumption and battery status, adaptively adjusting mission parameters to ensure sufficient energy remains for safe operation and landing. In critical low-battery scenarios, the architecture triggers automatic mission simplification, reducing computational workloads, constraining flight envelopes, and if necessary, executing emergency landing procedures at the nearest suitable location.

The safety and fault tolerance architecture implements multiple layers of protection against hardware failures, software errors, and environmental hazards. The system follows a defense-in-depth approach, with independent safety mechanisms operating at different levels of the architecture. Hardware redundancy includes duplicate sensors for critical measurements such as attitude and altitude, enabling cross-validation and continued operation if primary sensors fail. Software redundancy implements alternative algorithms for essential functions, such as maintaining basic stability using IMU-only processing if vision-based pose estimation becomes unavailable. Containment regions isolate critical processes, preventing failures in complex functions like object detection from affecting basic flight control systems. Supervisory monitoring continuously evaluates system health across multiple parameters, including sensor validity, algorithm performance, communication integrity, and battery status. When anomalies are detected, the system applies appropriate mitigations ranging from automatic reconfiguration to emergency response procedures. The architecture defines clear operational envelopes with geofencing constraints, maximum altitude and velocity limits, and minimum clearance requirements, actively preventing commands that would exceed safe operating parameters. In extreme circumstances, the system can execute fully autonomous emergency procedures, including return-to-home navigation, controlled descent, or emergency landing, depending on the nature and severity of the detected issues.

The human-machine interface balances autonomous operation with appropriate human oversight and intervention capabilities. The primary interface is implemented through a ground control station application based on customized QGroundControl software, providing intuitive visualization of the drone's status, sensor data, and mission progress. The interface follows a hierarchical design that presents the most critical information continuously while allowing operators to access progressively more detailed data as needed. Mission planning capabilities enable operators to define objectives at varying levels of abstraction, from specific waypoint sequences to high-level goals like "inspect this area" or "follow this structure," with the autonomous system handling the detailed execution. During operation, the interface provides multiple monitoring views, including real-time camera feeds with augmented reality overlays highlighting detected objects and planned paths, 3D map displays showing the drone's position and environmental model, and status panels presenting critical telemetry and system health indicators. Intervention mechanisms allow operators to modify missions in progress, adjust parameters such as flight speed or altitude, manually control the drone when necessary, or trigger predefined emergency procedures. The interface adapts to different operator expertise levels, providing simplified controls for basic users while allowing experts to access advanced configurations and detailed system information.

The development, testing, and deployment methodology associated with the system architecture emphasizes incremental validation and continuous integration. The development process follows a spiral model with increasingly complex prototypes, beginning with simulation-only implementations that validate basic algorithms and interactions, progressing to hardware-in-the-loop testing that combines real hardware with simulated environments, and culminating in full field testing under controlled conditions. Modular testing frameworks enable automated validation of individual components against specified performance criteria, while integration testing verifies correct interaction between subsystems. The continuous integration pipeline automatically rebuilds the system when changes are committed, executing unit tests, static analysis, and when applicable, simulation-based validation to identify regressions or integration issues. Deployment procedures include systematic pre-flight checks that verify sensor calibration, system responsiveness, and communication integrity before authorizing mission execution. The architecture incorporates built-in monitoring and logging throughout development and operation, capturing performance metrics and failure indicators to inform ongoing system improvement. This methodical approach to development and testing ensures that the autonomous drone system meets its functional requirements while maintaining the reliability and safety characteristics necessary for practical deployment.

The architectural decisions detailed in this chapter directly address several research gaps identified in the literature review, particularly those related to system integration and real-time performance in resource-constrained environments. The modular, layered design enables independent development and testing of components while ensuring their effective integration into a cohesive system. The mixed-criticality approach with explicit failure handling pathways enhances system reliability across varying conditions. The multi-modal sensor configuration with cross-validation capabilities improves environmental perception robustness. The adaptive power management system optimizes energy usage to extend operational duration. Together, these architectural elements create a foundation for the perception, navigation, and obstacle avoidance capabilities detailed in subsequent chapters, enabling the autonomous drone system to operate reliably in complex, dynamic environments while meeting the strict resource constraints inherent to aerial platforms. 