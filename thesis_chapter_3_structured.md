# Chapter 3: System Architecture

## 3.1 Architectural Overview

The autonomous drone system architecture follows a modular, layered design approach that enables component isolation, simplified testing, and future expandability. This architectural philosophy prioritizes clear interfaces between subsystems, allowing individual components to be developed, tested, and refined independently while maintaining compatibility with the overall system. The modular approach also facilitates future enhancements by allowing specific components to be upgraded or replaced without requiring wholesale system redesign. This chapter details the hardware platform, computational architecture, sensor configuration, software organization, and communication framework that collectively form the foundation of the autonomous drone system. The architectural decisions described here directly address several research gaps identified in the literature review, particularly those related to system integration and real-time performance in resource-constrained environments.

## 3.2 Hardware Platform

The hardware platform selection represents a critical foundation for the autonomous drone system, balancing computational capabilities against power consumption, weight, and cost constraints. After evaluating several embedded computing platforms, the Jetson Xavier NX was selected as the primary computing unit due to its optimal combination of GPU-accelerated processing power (384 CUDA cores and 48 Tensor cores), moderate power consumption (10-15W during typical operation), and reasonable weight (approximately 84g for the module alone). The platform provides 8GB of RAM and 16GB of eMMC storage, sufficient for running the perception, planning, and control algorithms without external memory requirements. The Xavier NX's hardware acceleration capabilities are particularly valuable for the computer vision components of the system, enabling real-time object detection while maintaining acceptable power consumption. The computing platform is integrated with a custom carrier board that provides power management, sensor interfaces, and communication links optimized for the drone application, reducing weight and complexity compared to standard development carrier boards.

The drone platform itself is based on a modified quadcopter frame with a 450mm motor-to-motor diagonal distance, providing sufficient payload capacity (approximately 800g beyond the base weight) while maintaining agility and reasonable flight time. The frame features a central mounting area for the computing system and primary sensors, with dedicated vibration-isolated platforms for inertial measurement units and sensitive camera equipment. Power is supplied by a 4S 5000mAh LiPo battery, providing approximately 20 minutes of flight time under typical operation conditions. The propulsion system consists of four 920KV brushless motors with 10-inch propellers, selected to balance thrust requirements (approximately 1.2kg per motor) with energy efficiency. Multiple power distribution systems separate the high-current motor circuits from the sensitive computing and sensor systems, reducing electromagnetic interference and providing stable, filtered power to critical components. The platform includes automatic battery monitoring with configurable low-voltage thresholds that trigger landing procedures when capacity falls below safe operating levels.

## 3.3 Sensor Configuration

The sensor configuration integrates multiple modalities to create a comprehensive environmental perception system resistant to individual sensor limitations or failures. The primary visual perception system consists of three global-shutter cameras: a forward-facing 120° wide-angle camera for general navigation and obstacle detection, a downward-facing camera for ground tracking and landing area assessment, and a 30° narrow-angle forward camera for detailed inspection and long-range obstacle identification. This multi-camera approach addresses the trade-off between field of view and resolution, providing both broad environmental awareness and detailed object recognition capabilities. 

Depth perception is achieved through a combination of stereo vision processing between overlapping camera fields and a dedicated depth camera based on structured light technology, which provides reliable short to medium-range 3D information within a 60° field of view. Inertial sensing employs a primary 9-axis IMU (accelerometer, gyroscope, and magnetometer) for attitude estimation and motion tracking, with a secondary redundant IMU that enables consistency checking and fault detection. Global positioning integrates GPS/GNSS receivers with barometric pressure sensors for altitude estimation, creating a reliable positioning system with approximately ±2m horizontal accuracy and ±0.5m relative altitude accuracy in typical conditions.

Additional sensing capabilities include ultrasonic rangefinders providing precise altitude measurement during takeoff and landing phases, infrared time-of-flight sensors for close-range obstacle detection in low-light conditions, and ambient light sensors that trigger camera parameter adjustments based on environmental illumination. The sensor suite is carefully positioned to minimize interference between components while maximizing coverage of the surrounding environment, with particular attention to forward and downward sensing for the most common flight scenarios.

## 3.4 Software Architecture

The software architecture implements a hierarchical structure with distinct layers for hardware abstraction, perception, planning, and control, facilitating clear separation of concerns and modular development. The lowest layer, the Hardware Abstraction Layer (HAL), provides standardized interfaces to sensors and actuators, isolating higher-level components from hardware-specific details and enabling hardware changes without modifying application code. The Perception Layer processes sensor data to create environmental representations, including object detection through the YOLOv8 vision system, depth mapping from stereo and structured light sensors, and state estimation combining inertial and visual odometry data. The Planning Layer includes both global path planning based on mission objectives and local trajectory generation that considers immediate obstacles and constraints. The Control Layer implements nested control loops for position, velocity, and attitude stabilization, translating high-level trajectory commands into specific motor control signals. 

This layered architecture enables clear interfaces between components while allowing each layer to operate at appropriate update frequencies, from the 400Hz attitude control loop to the 10Hz global planning update rate. Each layer communicates with adjacent layers through well-defined interfaces that specify both data formats and timing requirements, creating a modular system where individual components can be modified or replaced without disrupting the overall architecture. The software implementation uses a combination of C++ for performance-critical components and Python for higher-level functionality, with ROS2 middleware providing standardized communication between components.

The system implements a mixed-criticality approach, with safety-critical functions receiving prioritized resources and guaranteed execution times. The flight controller firmware handles the most critical tasks, including attitude stabilization and basic obstacle avoidance, operating in a real-time environment with deterministic performance. The companion computer (Jetson Xavier NX) manages higher-level functions including vision processing, path planning, and mission management, with critical processes assigned higher scheduling priorities and, where necessary, dedicated CPU cores. This division of responsibilities ensures that basic flight safety is maintained even if the more complex perception and planning systems experience delays or failures.

## 3.5 Communication Framework

The communication architecture establishes reliable data exchange between system components across multiple physical and logical channels. Internal communication between software components utilizes a publish-subscribe messaging framework based on ROS2 (Robot Operating System 2), selected for its flexible communication patterns and quality-of-service options that support both reliable and best-effort message delivery as appropriate for different data types. Critical control messages use deterministic communication paths with guaranteed bandwidth and bounded latency, while high-volume sensor data like camera feeds use best-effort delivery with efficient zero-copy methods where possible. 

External communication with the ground control station operates across redundant channels, including a primary 5.8GHz wireless link for high-bandwidth data and control, and a secondary 900MHz long-range link for basic telemetry and emergency commands. Both channels implement the MAVLink protocol for standardized message formatting and handling, with custom extensions for application-specific functionality. The communication system includes automatic channel selection based on signal quality, range requirements, and bandwidth needs, with graceful degradation as signal quality decreases. Encryption is applied to all control channels to prevent unauthorized access or interference, with AES-256 encryption for sensitive commands and lighter encryption for high-bandwidth video streams to balance security and performance.

Inter-process communication within the companion computer uses a combination of shared memory for high-bandwidth, low-latency data exchange between closely coupled components and message-passing for more loosely coupled subsystems. This hybrid approach balances performance considerations against modularity and fault isolation, enabling efficient data sharing while maintaining appropriate separation between components. The communication framework includes comprehensive monitoring and diagnostics capabilities, automatically detecting and reporting issues with latency, bandwidth, or message loss across all channels.

## 3.6 Data Management

The data management subsystem handles the collection, processing, storage, and distribution of information throughout the autonomous drone system. Sensor data undergoes multi-stage processing, beginning with hardware-level filtering and preprocessing (such as image debayering and initial noise reduction), followed by feature extraction and fusion with complementary sensors, and culminating in high-level representations such as object bounding boxes or occupancy maps. The system implements a multi-resolution approach to environmental representation, with detailed high-resolution mapping for the immediate surroundings (within 10 meters) and progressively coarser representations for more distant areas, balancing memory usage and computational requirements against the precision needed at different ranges. 

Persistent storage manages both operational data, such as maps and environment models, and system logs that record flight parameters, decisions, and sensor readings for post-mission analysis and system refinement. A standardized logging framework captures structured data with accurate timestamps, facilitating both automated analysis and manual inspection when necessary. The data management system also implements selective recording based on mission phases and detected events, automatically increasing the detail and frequency of data capture during complex maneuvers, anomalous conditions, or operator-flagged segments.

The environmental model maintains a unified representation combining information from multiple sensors and persisting it across time, creating a consistent understanding of the world that supports planning and decision-making. This model includes both geometric information about obstacle positions and semantic information about object types and properties, enabling more sophisticated reasoning about the environment than would be possible with raw sensor data alone. The model implements efficient update mechanisms that incorporate new information while maintaining consistency with previous observations, resolving conflicts between sensors through confidence-weighted integration.

## 3.7 Power Management

The power management system optimizes energy usage while ensuring reliable operation throughout the mission duration. Hardware-level power management includes dynamic voltage and frequency scaling for the computing platform, enabling performance adjustments based on current processing requirements. During computationally intensive operations, such as running multiple simultaneous detection models, the system allocates maximum power to the CPU and GPU cores; during less demanding phases, it reduces clock speeds and disables unnecessary processing units to extend battery life. 

Sensor management selectively activates and deactivates perception sensors based on mission requirements and environmental conditions, for example, reducing camera frame rates in stable, obstacle-free environments or activating long-range sensors only when planning high-speed segments. Software-level optimizations include workload scheduling that balances processing demands across time, avoiding simultaneous execution of multiple intensive tasks when possible. The system continuously monitors power consumption and battery status, adaptively adjusting mission parameters to ensure sufficient energy remains for safe operation and landing. In critical low-battery scenarios, the architecture triggers automatic mission simplification, reducing computational workloads, constraining flight envelopes, and if necessary, executing emergency landing procedures at the nearest suitable location.

The power distribution system separates critical and non-critical circuits, ensuring that degraded battery performance impacts lower-priority functions first while maintaining essential capabilities as long as possible. Power consumption monitoring tracks usage at both component and system levels, enabling detailed analysis of energy requirements across different mission phases and operating conditions. This information feeds into mission planning algorithms that consider energy constraints when generating routes and setting flight parameters, optimizing for efficiency while ensuring sufficient reserves for safe operation.

## 3.8 Safety and Fault Tolerance

The safety and fault tolerance architecture implements multiple layers of protection against hardware failures, software errors, and environmental hazards. The system follows a defense-in-depth approach, with independent safety mechanisms operating at different levels of the architecture. Hardware redundancy includes duplicate sensors for critical measurements such as attitude and altitude, enabling cross-validation and continued operation if primary sensors fail. Software redundancy implements alternative algorithms for essential functions, such as maintaining basic stability using IMU-only processing if vision-based pose estimation becomes unavailable. 

Containment regions isolate critical processes, preventing failures in complex functions like object detection from affecting basic flight control systems. Supervisory monitoring continuously evaluates system health across multiple parameters, including sensor validity, algorithm performance, communication integrity, and battery status. When anomalies are detected, the system applies appropriate mitigations ranging from automatic reconfiguration to emergency response procedures. The architecture defines clear operational envelopes with geofencing constraints, maximum altitude and velocity limits, and minimum clearance requirements, actively preventing commands that would exceed safe operating parameters. In extreme circumstances, the system can execute fully autonomous emergency procedures, including return-to-home navigation, controlled descent, or emergency landing, depending on the nature and severity of the detected issues.

The fault detection and recovery system incorporates both analytical and heuristic approaches. Analytical methods use mathematical models to detect deviations from expected behavior, while heuristic approaches apply rules derived from domain knowledge and experience. This combination provides robust detection across a wide range of failure modes, from obvious sensor malfunctions to subtle algorithmic errors. Recovery strategies follow a tiered approach, beginning with attempts to resolve issues through reconfiguration or parameter adjustment, then progressively falling back to simpler but more reliable operational modes if issues persist. The safety architecture undergoes rigorous validation through fault injection testing, deliberately introducing failures to verify appropriate detection and response.

## 3.9 Human-Machine Interface

The human-machine interface balances autonomous operation with appropriate human oversight and intervention capabilities. The primary interface is implemented through a ground control station application based on customized QGroundControl software, providing intuitive visualization of the drone's status, sensor data, and mission progress. The interface follows a hierarchical design that presents the most critical information continuously while allowing operators to access progressively more detailed data as needed. 

Mission planning capabilities enable operators to define objectives at varying levels of abstraction, from specific waypoint sequences to high-level goals like "inspect this area" or "follow this structure," with the autonomous system handling the detailed execution. During operation, the interface provides multiple monitoring views, including real-time camera feeds with augmented reality overlays highlighting detected objects and planned paths, 3D map displays showing the drone's position and environmental model, and status panels presenting critical telemetry and system health indicators. 

Intervention mechanisms allow operators to modify missions in progress, adjust parameters such as flight speed or altitude, manually control the drone when necessary, or trigger predefined emergency procedures. The interface adapts to different operator expertise levels, providing simplified controls for basic users while allowing experts to access advanced configurations and detailed system information. Communication between the interface and the drone uses a secure, authenticated connection with encryption to prevent unauthorized access, ensuring that only authorized operators can control or monitor the system.

The user interface design emphasizes situation awareness, providing operators with a clear understanding of the drone's current state, intentions, and environmental context. Information presentation follows human factors best practices, using appropriate visualization techniques, consistent color coding, and graduated alerts that draw attention to important information without overwhelming the operator. The interface includes predictive elements that show anticipated trajectory and system states, helping operators understand not just what the drone is currently doing but what it plans to do next, improving their ability to identify potential issues before they become critical. 